# V2 LiveKit Integration

## Overview

V2 uses the official `@livekit/components-react` SDK instead of a custom 1260-line `useLiveKit` hook. This document covers:

1. **SDK components used** (`LiveKitRoom`, `RoomAudioRenderer`, `useRoomContext`)
2. **Token generation** (server-side in `page.tsx`)
3. **Room metadata updates** (when lesson changes)
4. **Data channel communication** (agent transcripts, FA responses, quiz evaluations)
5. **Voice mode** (enable/disable microphone + agent STT)
6. **Audio mute sync** (global audio toggle)

## Architecture Comparison

| Aspect | V1 (`useLiveKit` hook) | V2 (SDK Components) |
|--------|------------------------|---------------------|
| Lines of code | 1260 lines | ~50 lines (spread across components) |
| Connection | Manual via `Room.connect()` | `<LiveKitRoom connect={true} />` |
| Audio playback | Manual track attachment | `<RoomAudioRenderer />` automatic |
| Room access | `roomRef.current` | `useRoomContext()` |
| Token fetch | Client-side `useEffect` | Server-side in `page.tsx` |
| Data channel | Manual listeners on room | Same, but cleaner hook structure |

## SDK Components Used

```bash
bun add @livekit/components-react
```

### 1. `LiveKitRoom` - Connection Wrapper

```typescript
import { LiveKitRoom } from "@livekit/components-react";

<LiveKitRoom
  serverUrl={process.env.NEXT_PUBLIC_LIVEKIT_URL}
  token={liveKitToken}      // Pre-generated by server
  connect={true}            // Connect on mount
  audio={false}             // Mic off by default
  video={false}             // No video
  options={{
    adaptiveStream: true,
    dynacast: true,
  }}
>
  {children}
</LiveKitRoom>
```

### 2. `RoomAudioRenderer` - Automatic Audio Playback

```typescript
import { RoomAudioRenderer } from "@livekit/components-react";

// Inside LiveKitRoom, handles all agent audio playback automatically
<RoomAudioRenderer />
```

**What it does:**
- Subscribes to all audio tracks from remote participants (agent)
- Creates audio elements and manages playback
- Handles browser autoplay policies
- No manual track attachment needed

### 3. `useRoomContext` - Access Room Instance

```typescript
import { useRoomContext } from "@livekit/components-react";

function MyComponent() {
  const room = useRoomContext();
  
  // Access room properties
  const isConnected = room.state === "connected";
  
  // Access local participant
  const localParticipant = room.localParticipant;
  
  // Send data
  await room.localParticipant.publishData(data, { reliable: true });
  
  // Send text message to agent
  await room.localParticipant.sendText(text, { topic: "lk.chat" });
}
```

### 4. `useVoiceAssistant` - Voice Agent State (Official)

```typescript
import { useVoiceAssistant, BarVisualizer } from "@livekit/components-react";

function VoiceAssistantUI() {
  // Get agent's audio track and current state
  const { state, audioTrack } = useVoiceAssistant();
  
  return (
    <div>
      <BarVisualizer state={state} barCount={5} trackRef={audioTrack} />
      <p>{state}</p> {/* "listening", "thinking", "speaking", etc. */}
    </div>
  );
}
```

**What it provides:**
- `state` - Current agent state ("listening", "thinking", "speaking", "idle")
- `audioTrack` - Agent's audio track reference for visualization
- `agent` - The agent participant object

### 5. `useTextStream` - Text Streaming (Official)

```typescript
import { useTextStream } from "@livekit/components-react";

function TranscriptDisplay() {
  // Subscribe to transcription topic
  const { textStreams } = useTextStream("lk.transcription");
  
  return (
    <div>
      {textStreams.map((stream) => (
        <p key={stream.id}>{stream.text}</p>
      ))}
    </div>
  );
}
```

**Note:** For fine-grained control over different message types, we use manual `room.on("dataReceived")` and `room.registerTextStreamHandler()`.

## Provider Hierarchy

```
page.tsx (Server Component)
└── ModuleView (Client Component)
    └── ModuleProvider (course, module, userId, activeLesson)
        └── LiveKitRoom (token, serverUrl, connect)
            └── RoomAudioRenderer
            └── MessagesProvider (composes useChat + useQuiz)
                └── RoomMetadataUpdater
                └── ModuleLayout
                    └── ChatPanel
                    └── VideoPanel
```

## Token Generation (Server-Side)

### Location: `app/(learning)/v2/course/[courseId]/module/[moduleId]/page.tsx`

```typescript
import { getLiveKitToken } from "@/actions/livekit";

export default async function ModulePageV2({ params }: ModulePageProps) {
  // ... auth, data fetching ...

  const roomName = `${course.id}-${module.id}`;
  
  const liveKitToken = await getLiveKitToken({
    roomName,
    participantName: session.user.id,
    metadata: {
      agent_type: "bodh-agent",
      courseId: course.id,
      courseTitle: course.title,
      moduleId: module.id,
      moduleTitle: module.title,
      lessonId: initialLesson?.id,
      lessonTitle: initialLesson?.title,
      lessonNumber: sessionTypeData.lessonNumber,
      videoIds: initialLesson?.kpointVideoId ? [initialLesson.kpointVideoId] : [],
      learningObjectives: course.learningObjectives,
      userId: session.user.id,
      userName: session.user.name,
      sessionType: sessionTypeData.sessionType,
      isFirstCourseVisit: sessionTypeData.isFirstCourseVisit,
      isIntroLesson: sessionTypeData.isIntroLesson,
      prevLessonTitle: sessionTypeData.prevLessonTitle,
    },
  });

  if (!liveKitToken) {
    redirect("/error?reason=livekit_token_failed");
  }

  return <ModuleView liveKitToken={liveKitToken} roomName={roomName} ... />;
}
```

### Server Action: `actions/livekit.ts`

```typescript
"use server";

import { AccessToken, type VideoGrant } from "livekit-server-sdk";
import { roomService, getLiveKitCredentials } from "@/lib/livekit";

export async function getLiveKitToken({
  roomName,
  participantName,
  metadata = {},
}: GetLiveKitTokenParams): Promise<string | null> {
  try {
    const { apiKey, apiSecret, url } = getLiveKitCredentials();

    // Create or update room with metadata
    try {
      await roomService.createRoom({
        name: roomName,
        metadata: JSON.stringify(metadata),
        emptyTimeout: 600,
      });
    } catch {
      // Room exists - update metadata
      await roomService.updateRoomMetadata(roomName, JSON.stringify(metadata));
    }

    // Create access token
    const token = new AccessToken(apiKey, apiSecret, {
      identity: participantName,
      name: participantName,
      metadata: JSON.stringify(metadata),
    });

    token.addGrant({
      roomJoin: true,
      room: roomName,
      canPublish: true,
      canSubscribe: true,
      canPublishData: true,
    } as VideoGrant);

    return await token.toJwt();
  } catch (error) {
    console.error("[getLiveKitToken] Error:", error);
    return null;
  }
}

export async function updateRoomMetadata(
  roomName: string,
  metadata: RoomMetadata
): Promise<void> {
  try {
    await roomService.updateRoomMetadata(roomName, JSON.stringify(metadata));
  } catch (error) {
    console.error("[updateRoomMetadata] Error:", error);
  }
}
```

## Room Metadata Updates

When user changes lesson (sidebar click, auto-advance, action button):

### Component: `RoomMetadataUpdater`

```typescript
function RoomMetadataUpdater({
  roomName,
  course,
  module,
  activeLesson,
  sessionType,
}: Props) {
  const room = useRoomContext();
  const prevLessonIdRef = useRef<string | undefined>(undefined);

  useEffect(() => {
    // Only update if lesson actually changed (not on first render)
    if (prevLessonIdRef.current && prevLessonIdRef.current !== activeLesson?.id) {
      updateRoomMetadata(roomName, {
        courseId: course.id,
        courseTitle: course.title,
        moduleId: module.id,
        moduleTitle: module.title,
        lessonId: activeLesson?.id,
        lessonTitle: activeLesson?.title,
        videoIds: activeLesson?.kpointVideoId ? [activeLesson.kpointVideoId] : [],
        learningObjectives: course.learningObjectives,
        sessionType: sessionType.sessionType,
      }).catch(console.error);
    }

    prevLessonIdRef.current = activeLesson?.id;
  }, [activeLesson?.id, roomName, course, module, sessionType]);

  return null; // Render nothing - just side effects
}
```

## Data Channel Communication

### Location: `MessagesProvider.tsx`

The data channel handles multiple message types from the agent:

```typescript
function MessagesProvider({ children }: Props) {
  const room = useRoomContext();

  useEffect(() => {
    if (!room) return;

    const handleDataReceived = (
      payload: Uint8Array,
      participant?: any,
      kind?: any,
      topic?: string
    ) => {
      try {
        const decoder = new TextDecoder();
        const text = decoder.decode(payload);
        const data = JSON.parse(text);

        // Route based on message type
        switch (data.type) {
          case "fa_response":
            // FA question/feedback from agent
            quiz.handleFAResponse(data);
            break;

          case "quiz_evaluation_result":
            // Text quiz evaluation from Sarvam
            handleQuizEvaluation(data);
            break;

          case "user_transcription":
            // User speech transcription (voice mode)
            handleUserTranscription(data);
            break;

          case "fa_intro_complete":
            // FA intro finished, show start button
            handleFAIntroComplete(data);
            break;

          case "voice_mode_changed":
            // Agent confirmed voice mode toggle
            setIsVoiceModeEnabled(data.enabled);
            break;

          case "agent_response":
            // TTS fallback when audio fails
            if (data.tts_failed) {
              handleTTSFallback(data);
            }
            break;
        }
      } catch {
        // Not JSON - ignore
      }
    };

    room.on("dataReceived", handleDataReceived);
    return () => room.off("dataReceived", handleDataReceived);
  }, [room]);

  // ... rest of provider
}
```

### Message Types from Agent

| Type | Description | Data Fields |
|------|-------------|-------------|
| `fa_response` | FA question or feedback | `question_number`, `question_text`, `options`, `is_mcq`, `feedback_type`, `is_complete` |
| `quiz_evaluation_result` | Text quiz result | `questionId`, `isCorrect`, `feedback` |
| `user_transcription` | Voice transcription | `text`, `is_final` |
| `fa_intro_complete` | FA intro done | `topic`, `intro_message`, `tts_failed` |
| `voice_mode_changed` | Voice mode toggle | `enabled` |
| `agent_response` | TTS fallback | `text`, `tts_failed` |
| `user_message` | Auto-triggered message | `text`, `task_type` |

## Sending Messages to Agent

### Text Chat (lk.chat topic)

```typescript
function useSendToAgent() {
  const room = useRoomContext();

  const sendText = useCallback(async (text: string) => {
    if (!room?.localParticipant) return;
    
    await room.localParticipant.sendText(text, { topic: "lk.chat" });
  }, [room]);

  return { sendText };
}
```

### Data Channel (custom messages)

```typescript
function usePublishData() {
  const room = useRoomContext();

  const publishData = useCallback(async (data: object) => {
    if (!room?.localParticipant) return;
    
    const encoder = new TextEncoder();
    const payload = encoder.encode(JSON.stringify(data));
    await room.localParticipant.publishData(payload, { reliable: true });
  }, [room]);

  return { publishData };
}
```

### Example: Send FA Answer

```typescript
// In useQuiz hook
const submitFAAnswer = useCallback(async (answer: string) => {
  await publishData({
    type: "fa_answer",
    questionId: currentQuestion.id,
    answer,
  });
}, [publishData, currentQuestion]);
```

## Voice Mode

Voice mode enables user microphone + agent STT (speech-to-text).

### Enable Voice Mode

```typescript
function useVoiceMode() {
  const room = useRoomContext();
  const [isVoiceModeEnabled, setIsVoiceModeEnabled] = useState(false);

  const enableVoiceMode = useCallback(async () => {
    if (!room) return false;

    // 1. Enable local microphone
    await room.localParticipant.setMicrophoneEnabled(true);

    // 2. Find agent participant
    let agentIdentity: string | null = null;
    room.remoteParticipants.forEach((p) => {
      if (p.isAgent || p.identity.includes("agent")) {
        agentIdentity = p.identity;
      }
    });

    if (!agentIdentity) return false;

    // 3. Call RPC to enable STT on agent
    const response = await room.localParticipant.performRpc({
      destinationIdentity: agentIdentity,
      method: "enable_voice_mode",
      payload: JSON.stringify({}),
    });

    const result = JSON.parse(response);
    if (result.success) {
      setIsVoiceModeEnabled(true);
      return true;
    }
    return false;
  }, [room]);

  const disableVoiceMode = useCallback(async () => {
    if (!room) return false;

    // 1. Disable local microphone
    await room.localParticipant.setMicrophoneEnabled(false);

    // 2. Call RPC to disable STT on agent
    // ... similar to enable

    setIsVoiceModeEnabled(false);
    return true;
  }, [room]);

  return { isVoiceModeEnabled, enableVoiceMode, disableVoiceMode };
}
```

### Voice Mode Data Flow

```
User speaks → Microphone → LiveKit → Agent
                                      ↓
                            Agent STT (Sarvam)
                                      ↓
                           user_transcription
                                      ↓ (data channel)
                                   Client
                                      ↓
                               Display in UI
```

## Audio Output Mute (Global Toggle)

Sync with `AudioContext` provider for global mute control:

### Hook: `useLiveKitAudioSync`

```typescript
function useLiveKitAudioSync() {
  const room = useRoomContext();
  const { isMuted, registerCallback, unregisterCallback } = useAudioContext();

  useEffect(() => {
    if (!room) return;

    // Sync initial state
    setAgentAudioEnabled(!isMuted);

    // Listen for mute changes
    const handleMuteChange = (muted: boolean) => {
      setAgentAudioEnabled(!muted);
    };

    registerCallback(handleMuteChange);
    return () => unregisterCallback(handleMuteChange);
  }, [room, isMuted]);

  const setAgentAudioEnabled = useCallback(async (enabled: boolean) => {
    if (!room) return;

    // Find agent and call RPC
    let agentIdentity: string | null = null;
    room.remoteParticipants.forEach((p) => {
      if (p.isAgent) agentIdentity = p.identity;
    });

    if (agentIdentity) {
      await room.localParticipant.performRpc({
        destinationIdentity: agentIdentity,
        method: "set_audio_output",
        payload: JSON.stringify({ enabled }),
      });
    }
  }, [room]);
}
```

## Agent Transcript Streaming

The agent sends transcriptions via the `lk.transcription` text stream. Two approaches:

### Option A: Use `useTextStream` Hook (Simpler)

```typescript
import { useTextStream } from "@livekit/components-react";

function useAgentTranscript() {
  const { textStreams } = useTextStream("lk.transcription");
  
  // textStreams contains all transcript segments
  // Each has: id, text, participantIdentity, etc.
  return { transcripts: textStreams };
}
```

### Option B: Manual Handler (More Control)

Use when you need to differentiate between interim/final transcripts or handle segment attributes:

```typescript
function useAgentTranscript(onTranscript: (segment: TranscriptSegment) => void) {
  const room = useRoomContext();

  useEffect(() => {
    if (!room) return;

    room.registerTextStreamHandler(
      "lk.transcription",
      async (reader, participantIdentity) => {
        const info = reader.info;
        const attributes = info?.attributes || {};
        const isFinal = attributes["lk.transcription_final"] === "true";
        const segmentId = attributes["lk.segment_id"] || `seg-${Date.now()}`;
        const transcribedTrackId = attributes["lk.transcribed_track_id"];
        
        // Check if this is from agent (has transcribed track ID)
        const isFromAgent = !!transcribedTrackId;
        if (!isFromAgent) return;

        // Stream chunks incrementally
        let accumulatedText = "";
        for await (const chunk of reader) {
          accumulatedText += chunk;

          // Call callback with partial transcript
          onTranscript({
            id: segmentId,
            text: accumulatedText,
            participantIdentity,
            isAgent: true,
            isFinal: false,
            timestamp: Date.now(),
          });
        }

        // Mark as final when stream completes
        if (accumulatedText.trim()) {
          onTranscript({
            id: segmentId,
            text: accumulatedText,
            participantIdentity,
            isAgent: true,
            isFinal: true,
            timestamp: Date.now(),
          });
        }
      }
    );
  }, [room, onTranscript]);
}
```

### Transcript Stream Attributes

| Attribute | Description |
|-----------|-------------|
| `lk.transcription_final` | `"true"` if final segment |
| `lk.segment_id` | Unique segment identifier |
| `lk.transcribed_track_id` | Track ID being transcribed (present = agent audio) |

## Environment Variables

```bash
# Server-side (for token generation)
LIVEKIT_URL=wss://your-livekit-server.com
LIVEKIT_API_KEY=your-api-key
LIVEKIT_API_SECRET=your-api-secret

# Client-side (for SDK connection)
NEXT_PUBLIC_LIVEKIT_URL=wss://your-livekit-server.com
```

## Error Handling

### Connection Errors

```typescript
<LiveKitRoom
  onError={(error) => {
    console.error("[LiveKit] Connection error:", error);
    toast.error("Voice connection failed", {
      description: error.message,
    });
  }}
  onDisconnected={(reason) => {
    console.log("[LiveKit] Disconnected:", reason);
    // Optionally show reconnect UI
  }}
>
```

### Token Generation Failure

```typescript
// In page.tsx
if (!liveKitToken) {
  redirect("/error?reason=livekit_token_failed");
}
```

### Graceful Degradation

If LiveKit fails, the UI should still work:
- Chat becomes text-only (no voice TTS)
- Video panel still works (KPoint player is independent)
- Quiz questions work (DB-based quizzes don't need LiveKit)
- FA quizzes degrade (show error, suggest refresh)

## Files Created/Modified for V2 LiveKit

| File | Purpose |
|------|---------|
| `actions/livekit.ts` | Server actions for token + metadata |
| `lib/livekit.ts` | RoomServiceClient singleton |
| `ModuleView.tsx` | `LiveKitRoom` wrapper |
| `MessagesProvider.tsx` | Data channel listener |
| `hooks/useAgentTranscript.ts` | Transcript stream handler |
| `hooks/useVoiceMode.ts` | Voice mode toggle |
| `hooks/useLiveKitAudioSync.ts` | Global mute sync |

## V2 Implementation Decision: SDK vs Custom

### Recommended Approach: Hybrid

Use SDK components for connection/audio, but keep custom hooks for data channel:

| Feature | Approach | Reason |
|---------|----------|--------|
| Connection | `<LiveKitRoom>` | Simpler, handles reconnection |
| Audio playback | `<RoomAudioRenderer>` | Handles autoplay, track management |
| Room access | `useRoomContext()` | Standard hook pattern |
| Agent state | `useVoiceAssistant()` | Built-in state machine |
| Transcripts | Custom `registerTextStreamHandler` | Need segment attributes |
| Data channel | Custom `room.on("dataReceived")` | Route multiple message types |
| Voice mode | Custom RPC calls | App-specific agent methods |
| Audio mute sync | Custom hook | Sync with AudioContext provider |

### Why Not Pure SDK?

1. **Multiple message types** - We route FA responses, quiz evaluations, user transcriptions differently
2. **Custom RPC methods** - `enable_voice_mode`, `set_audio_output` are app-specific
3. **Segment attributes** - Need `lk.transcription_final`, `lk.segment_id` for proper UI
4. **AudioContext sync** - Global mute state needs to sync with LiveKit

### Why Not Pure Custom (V1)?

1. **1260 lines** - Too much boilerplate
2. **Manual track attachment** - `RoomAudioRenderer` handles this
3. **Connection state** - `LiveKitRoom` manages reconnection
4. **Browser autoplay** - SDK handles policies

## Migration Checklist

- [ ] Add `@livekit/components-react` and `@livekit/components-styles` to dependencies
- [ ] Move token generation to `page.tsx` server action
- [ ] Replace `useLiveKit` connection logic with `<LiveKitRoom>` + `<RoomAudioRenderer>`
- [ ] Update `MessagesProvider` with data channel listener using `room.on("dataReceived")`
- [ ] Add transcript handler using `room.registerTextStreamHandler("lk.transcription")`
- [ ] Add `RoomMetadataUpdater` component for lesson changes
- [ ] Create `useVoiceMode` hook with RPC calls
- [ ] Create `useLiveKitAudioSync` hook for global mute
- [ ] Optionally use `useVoiceAssistant` for agent state visualization
- [ ] Test all data channel message types
- [ ] Verify graceful degradation when LiveKit unavailable

## Package Installation

```bash
bun add @livekit/components-react @livekit/components-styles
```

The project already has `livekit-client` and `livekit-server-sdk` installed.
